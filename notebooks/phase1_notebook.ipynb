{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRM Customer Churn Analysis - Phase 1 Complete Tutorial\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of Phase 1 of the CRM Analytics Pipeline:\n",
    "\n",
    "1. **Data Collection**: Load and simulate customer data from multiple sources\n",
    "2. **Data Cleaning**: Handle missing values, outliers, and data quality issues\n",
    "3. **Data Validation**: Ensure data meets quality standards\n",
    "4. **Feature Engineering**: Create meaningful features from raw data\n",
    "5. **Exploratory Data Analysis**: Understand patterns and relationships\n",
    "6. **Model Preparation**: Prepare datasets for Phase 2 modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:13:23.652385Z",
     "start_time": "2025-10-01T06:13:21.709364Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"📁 Project root: {project_root}\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Import Project Modules\n",
    "\n",
    "Now let's import our custom modules from the project."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:13:30.125763Z",
     "start_time": "2025-10-01T06:13:23.677029Z"
    }
   },
   "source": [
    "# Import project modules\n",
    "from config.settings import get_settings\n",
    "from src.data.collector import DataCollector\n",
    "from src.data.cleaner import DataCleaner\n",
    "from src.data.validator import DataValidator\n",
    "from src.features.engineer import FeatureEngineer\n",
    "from src.analysis.eda import ExploratoryDataAnalysis\n",
    "from src.pipeline.orchestrator import PipelineOrchestrator, PipelineStage\n",
    "\n",
    "# Initialize settings\n",
    "settings = get_settings()\n",
    "\n",
    "print(\"✅ Project modules imported successfully!\")\n",
    "print(f\"📂 Data directory: {settings.paths.DATA_DIR}\")\n",
    "print(f\"📊 Reports directory: {settings.paths.REPORTS_DIR}\")"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1️⃣ Data Collection\n",
    "\n",
    "## Understanding the Data Sources\n",
    "\n",
    "Our CRM system collects data from four main sources:\n",
    "\n",
    "1. **Customers**: Demographics and account information\n",
    "2. **Transactions**: Purchase history and behavior\n",
    "3. **Interactions**: Customer service touchpoints\n",
    "4. **Marketing**: Campaign engagement data\n",
    "\n",
    "Let's collect and explore each dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-01T06:13:30.169462Z"
    }
   },
   "source": [
    "# Initialize the data collector\n",
    "collector = DataCollector(settings)\n",
    "\n",
    "# Collect data from all sources\n",
    "# Note: This will use simulated data if database is unavailable\n",
    "print(\"🔄 Starting data collection...\")\n",
    "print(\"⏱️  This may take a few minutes...\\n\")\n",
    "\n",
    "raw_data = collector.collect_all_data(\n",
    "    sources=['customers', 'transactions', 'interactions', 'marketing'],\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Data collection complete!\")\n",
    "print(\"\\n📊 Dataset Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, df in raw_data.items():\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "    print(f\"\\n{name.upper():15s} | Rows: {len(df):>8,} | Columns: {len(df.columns):>3} | Memory: {memory_mb:>7.2f} MB\")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Explore Customer Data\n",
    "\n",
    "Let's take a detailed look at the customer dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:00.891644Z",
     "start_time": "2025-10-01T06:15:00.879652Z"
    }
   },
   "source": [
    "customers_df = raw_data['customers']\n",
    "\n",
    "print(\"👥 CUSTOMER DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Customers: {len(customers_df):,}\")\n",
    "print(f\"\\nColumn Names and Types:\")\n",
    "print(customers_df.dtypes)\n",
    "\n",
    "print(\"\\n📋 Sample Records:\")\n",
    "display(customers_df.head(10))\n",
    "\n",
    "print(\"\\n📊 Statistical Summary:\")\n",
    "display(customers_df.describe())\n",
    "\n",
    "print(\"\\n❓ Missing Values:\")\n",
    "missing = customers_df.isnull().sum()\n",
    "missing_pct = (missing / len(customers_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Demographics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:00.897034Z",
     "start_time": "2025-10-01T06:15:00.896620Z"
    }
   },
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Customer Demographics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Age distribution\n",
    "customers_df['age'].hist(bins=30, ax=axes[0, 0], edgecolor='black')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Gender distribution\n",
    "customers_df['gender'].value_counts().plot(kind='bar', ax=axes[0, 1], color=['skyblue', 'pink', 'lightgreen'])\n",
    "axes[0, 1].set_title('Gender Distribution')\n",
    "axes[0, 1].set_xlabel('Gender')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Customer Segment\n",
    "customers_df['customer_segment'].value_counts().plot(kind='bar', ax=axes[0, 2], color='coral')\n",
    "axes[0, 2].set_title('Customer Segments')\n",
    "axes[0, 2].set_xlabel('Segment')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Acquisition Channel\n",
    "customers_df['acquisition_channel'].value_counts().plot(kind='barh', ax=axes[1, 0], color='lightblue')\n",
    "axes[1, 0].set_title('Acquisition Channels')\n",
    "axes[1, 0].set_xlabel('Count')\n",
    "axes[1, 0].set_ylabel('Channel')\n",
    "\n",
    "# Churn Distribution\n",
    "churn_counts = customers_df['churned'].value_counts()\n",
    "axes[1, 1].pie(churn_counts.values, labels=['Not Churned', 'Churned'], \n",
    "               autopct='%1.1f%%', colors=['green', 'red'], startangle=90)\n",
    "axes[1, 1].set_title('Churn Distribution')\n",
    "\n",
    "# State Distribution (Top 10)\n",
    "customers_df['state'].value_counts().head(10).plot(kind='bar', ax=axes[1, 2], color='purple')\n",
    "axes[1, 2].set_title('Top 10 States')\n",
    "axes[1, 2].set_xlabel('State')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📈 Key Insights:\")\n",
    "print(f\"  • Average age: {customers_df['age'].mean():.1f} years\")\n",
    "print(f\"  • Churn rate: {customers_df['churned'].mean()*100:.2f}%\")\n",
    "print(f\"  • Most common segment: {customers_df['customer_segment'].mode()[0]}\")\n",
    "print(f\"  • Top acquisition channel: {customers_df['acquisition_channel'].mode()[0]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Explore Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:00.938141Z",
     "start_time": "2025-10-01T06:15:00.937150Z"
    }
   },
   "source": [
    "transactions_df = raw_data['transactions']\n",
    "\n",
    "print(\"💳 TRANSACTION DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Transactions: {len(transactions_df):,}\")\n",
    "print(f\"Unique Customers: {transactions_df['customer_id'].nunique():,}\")\n",
    "print(f\"\\nTransaction Value Summary:\")\n",
    "print(f\"  • Total Revenue: ${transactions_df['total_amount'].sum():,.2f}\")\n",
    "print(f\"  • Average Transaction: ${transactions_df['total_amount'].mean():.2f}\")\n",
    "print(f\"  • Median Transaction: ${transactions_df['total_amount'].median():.2f}\")\n",
    "print(f\"  • Max Transaction: ${transactions_df['total_amount'].max():.2f}\")\n",
    "\n",
    "print(\"\\n📋 Sample Transactions:\")\n",
    "display(transactions_df.head(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:00.995698Z",
     "start_time": "2025-10-01T06:15:00.991390Z"
    }
   },
   "source": [
    "# Transaction Analytics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Transaction Analytics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Transaction amount distribution\n",
    "transactions_df['total_amount'].hist(bins=50, ax=axes[0, 0], edgecolor='black')\n",
    "axes[0, 0].set_title('Transaction Amount Distribution')\n",
    "axes[0, 0].set_xlabel('Amount ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_xlim(0, transactions_df['total_amount'].quantile(0.95))\n",
    "\n",
    "# Product category distribution\n",
    "if 'product_category' in transactions_df.columns:\n",
    "    transactions_df['product_category'].value_counts().plot(kind='bar', ax=axes[0, 1], color='orange')\n",
    "    axes[0, 1].set_title('Product Categories')\n",
    "    axes[0, 1].set_xlabel('Category')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Payment method distribution\n",
    "if 'payment_method' in transactions_df.columns:\n",
    "    transactions_df['payment_method'].value_counts().plot(kind='pie', ax=axes[1, 0], autopct='%1.1f%%')\n",
    "    axes[1, 0].set_title('Payment Methods')\n",
    "    axes[1, 0].set_ylabel('')\n",
    "\n",
    "# Channel distribution\n",
    "if 'channel' in transactions_df.columns:\n",
    "    transactions_df['channel'].value_counts().plot(kind='bar', ax=axes[1, 1], color='green')\n",
    "    axes[1, 1].set_title('Transaction Channels')\n",
    "    axes[1, 1].set_xlabel('Channel')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Explore Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:01.008581Z",
     "start_time": "2025-10-01T06:15:01.008175Z"
    }
   },
   "source": [
    "interactions_df = raw_data['interactions']\n",
    "\n",
    "print(\"📞 INTERACTION DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal Interactions: {len(interactions_df):,}\")\n",
    "print(f\"Unique Customers: {interactions_df['customer_id'].nunique():,}\")\n",
    "\n",
    "if 'satisfaction_score' in interactions_df.columns:\n",
    "    print(f\"\\nSatisfaction Metrics:\")\n",
    "    print(f\"  • Average Satisfaction: {interactions_df['satisfaction_score'].mean():.2f}/5\")\n",
    "    print(f\"  • Median Satisfaction: {interactions_df['satisfaction_score'].median():.0f}/5\")\n",
    "\n",
    "print(\"\\n📋 Sample Interactions:\")\n",
    "display(interactions_df.head(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Interaction Analytics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Customer Interaction Analytics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Interaction type distribution\n",
    "if 'interaction_type' in interactions_df.columns:\n",
    "    interactions_df['interaction_type'].value_counts().plot(kind='bar', ax=axes[0, 0], color='steelblue')\n",
    "    axes[0, 0].set_title('Interaction Types')\n",
    "    axes[0, 0].set_xlabel('Type')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Satisfaction score distribution\n",
    "if 'satisfaction_score' in interactions_df.columns:\n",
    "    interactions_df['satisfaction_score'].value_counts().sort_index().plot(kind='bar', ax=axes[0, 1], color='coral')\n",
    "    axes[0, 1].set_title('Satisfaction Scores')\n",
    "    axes[0, 1].set_xlabel('Score')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Channel distribution\n",
    "if 'channel' in interactions_df.columns:\n",
    "    interactions_df['channel'].value_counts().plot(kind='pie', ax=axes[1, 0], autopct='%1.1f%%')\n",
    "    axes[1, 0].set_title('Interaction Channels')\n",
    "    axes[1, 0].set_ylabel('')\n",
    "\n",
    "# Duration distribution\n",
    "if 'duration_seconds' in interactions_df.columns:\n",
    "    # Convert to minutes for better readability\n",
    "    (interactions_df['duration_seconds'] / 60).hist(bins=30, ax=axes[1, 1], edgecolor='black')\n",
    "    axes[1, 1].set_title('Interaction Duration')\n",
    "    axes[1, 1].set_xlabel('Duration (minutes)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_xlim(0, (interactions_df['duration_seconds'] / 60).quantile(0.95))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Explore Marketing Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "marketing_df = raw_data['marketing']\n",
    "\n",
    "print(\"📧 MARKETING DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(marketing_df) > 0:\n",
    "    print(f\"\\nTotal Campaigns: {len(marketing_df):,}\")\n",
    "    print(f\"Unique Customers Reached: {marketing_df['customer_id'].nunique():,}\")\n",
    "    \n",
    "    if 'opened' in marketing_df.columns:\n",
    "        print(f\"\\nEngagement Metrics:\")\n",
    "        print(f\"  • Open Rate: {marketing_df['opened'].mean()*100:.2f}%\")\n",
    "        print(f\"  • Click Rate: {marketing_df['clicked'].mean()*100:.2f}%\")\n",
    "        print(f\"  • Conversion Rate: {marketing_df['converted'].mean()*100:.2f}%\")\n",
    "    \n",
    "    display(marketing_df.head(10))\n",
    "else:\n",
    "    print(\"\\n⚠️  No marketing data available\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2️⃣ Data Cleaning\n",
    "\n",
    "## Understanding Data Quality Issues\n",
    "\n",
    "Before we can analyze data, we need to clean it. Common issues include:\n",
    "- Missing values\n",
    "- Duplicates\n",
    "- Outliers\n",
    "- Invalid data types\n",
    "- Business rule violations\n",
    "\n",
    "Let's clean our datasets!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize the data cleaner\n",
    "cleaner = DataCleaner(settings)\n",
    "\n",
    "print(\"🧹 Starting data cleaning...\")\n",
    "print(\"⏱️  This may take a minute...\\n\")\n",
    "\n",
    "# Clean all datasets\n",
    "cleaned_data = cleaner.clean_all_data(\n",
    "    data_dict=raw_data,\n",
    "    deep_clean=True\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Data cleaning complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Review Cleaning Report"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📊 CLEANING REPORT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, report in cleaner.cleaning_reports.items():\n",
    "    print(f\"\\n{dataset_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Records before: {report.total_records_before:,}\")\n",
    "    print(f\"Records after:  {report.total_records_after:,}\")\n",
    "    print(f\"Duplicates removed: {report.duplicates_removed:,}\")\n",
    "    \n",
    "    if report.missing_values_handled:\n",
    "        print(f\"\\nMissing values handled:\")\n",
    "        for col, count in report.missing_values_handled.items():\n",
    "            print(f\"  • {col}: {count:,}\")\n",
    "    \n",
    "    if report.outliers_handled:\n",
    "        print(f\"\\nOutliers handled:\")\n",
    "        for col, count in report.outliers_handled.items():\n",
    "            print(f\"  • {col}: {count:,}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Compare Before/After Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare customer data before and after cleaning\n",
    "print(\"📊 CUSTOMER DATA: BEFORE vs AFTER CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nBEFORE CLEANING:\")\n",
    "print(f\"Shape: {raw_data['customers'].shape}\")\n",
    "print(f\"Missing values: {raw_data['customers'].isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {raw_data['customers'].duplicated().sum()}\")\n",
    "\n",
    "print(\"\\nAFTER CLEANING:\")\n",
    "print(f\"Shape: {cleaned_data['customers'].shape}\")\n",
    "print(f\"Missing values: {cleaned_data['customers'].isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {cleaned_data['customers'].duplicated().sum()}\")\n",
    "\n",
    "print(\"\\nNew columns added:\")\n",
    "new_cols = set(cleaned_data['customers'].columns) - set(raw_data['customers'].columns)\n",
    "for col in new_cols:\n",
    "    print(f\"  • {col}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3️⃣ Data Validation\n",
    "\n",
    "## Quality Checks\n",
    "\n",
    "Let's validate our cleaned data to ensure it meets quality standards."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize validator\n",
    "validator = DataValidator(settings)\n",
    "\n",
    "print(\"🔍 Starting data validation...\\n\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for name, df in cleaned_data.items():\n",
    "    print(f\"\\nValidating {name}...\")\n",
    "    is_valid = validator.validate_data(df, dataset_name=name)\n",
    "    validation_results[name] = {\n",
    "        'valid': is_valid,\n",
    "        'summary': validator.validation_summary.copy()\n",
    "    }\n",
    "    \n",
    "print(\"\\n✅ Validation complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📊 VALIDATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for name, result in validation_results.items():\n",
    "    summary = result['summary']\n",
    "    summary_data.append({\n",
    "        'Dataset': name.upper(),\n",
    "        'Total Checks': summary['total_checks'],\n",
    "        'Passed': summary['passed'],\n",
    "        'Failed': summary['failed'],\n",
    "        'Errors': summary['errors'],\n",
    "        'Warnings': summary['warnings'],\n",
    "        'Pass Rate': f\"{summary['pass_rate']*100:.1f}%\",\n",
    "        'Status': '✅ Valid' if summary['is_valid'] else '⚠️ Issues Found'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4️⃣ Feature Engineering\n",
    "\n",
    "## Creating Meaningful Features\n",
    "\n",
    "Feature engineering is where we transform raw data into features that better represent the underlying problem.\n",
    "\n",
    "We'll create:\n",
    "1. **Transaction features**: Purchase patterns, frequency, monetary values\n",
    "2. **Interaction features**: Support engagement, satisfaction metrics\n",
    "3. **RFM features**: Recency, Frequency, Monetary analysis\n",
    "4. **Behavioral features**: Engagement scores, activity patterns\n",
    "5. **Time-based features**: Temporal patterns and trends"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer(settings)\n",
    "\n",
    "print(\"⚙️ Starting feature engineering...\")\n",
    "print(\"⏱️  This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Create features\n",
    "master_features = engineer.create_features(cleaned_data)\n",
    "\n",
    "print(\"\\n✅ Feature engineering complete!\")\n",
    "print(f\"\\n📊 Master Feature Set:\")\n",
    "print(f\"  • Total records: {len(master_features):,}\")\n",
    "print(f\"  • Total features: {len(master_features.columns):,}\")\n",
    "print(f\"  • Memory usage: {master_features.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Explore Generated Features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📋 FEATURE CATEGORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group features by category\n",
    "feature_categories = {\n",
    "    'Customer Demographics': [col for col in master_features.columns if any(x in col for x in ['age', 'gender', 'state'])],\n",
    "    'Transaction Features': [col for col in master_features.columns if any(x in col for x in ['transaction', 'spent', 'purchase'])],\n",
    "    'Interaction Features': [col for col in master_features.columns if any(x in col for x in ['interaction', 'satisfaction', 'support'])],\n",
    "    'RFM Features': [col for col in master_features.columns if any(x in col for x in ['recency', 'frequency', 'monetary', 'rfm'])],\n",
    "    'Behavioral Features': [col for col in master_features.columns if any(x in col for x in ['engagement', 'activity', 'behavior'])],\n",
    "    'Time Features': [col for col in master_features.columns if any(x in col for x in ['date', 'days', 'lifetime', 'age_'])]\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        print(f\"\\n{category} ({len(features)} features):\")\n",
    "        for feat in features[:10]:  # Show first 10\n",
    "            print(f\"  • {feat}\")\n",
    "        if len(features) > 10:\n",
    "            print(f\"  ... and {len(features) - 10} more\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show sample of master features\n",
    "print(\"\\n📊 Sample of Master Features:\")\n",
    "display(master_features.head(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if hasattr(engineer, 'feature_importance') and engineer.feature_importance is not None:\n",
    "    print(\"\\n🎯 TOP 20 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    top_features = engineer.feature_importance.head(20)\n",
    "    display(top_features)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title('Top 20 Most Important Features for Churn Prediction')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n⚠️  Feature importance not calculated\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5️⃣ Exploratory Data Analysis (EDA)\n",
    "\n",
    "## Understanding Patterns and Relationships\n",
    "\n",
    "Now let's dive deep into the data to understand patterns, correlations, and insights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:01.180199Z",
     "start_time": "2025-10-01T06:15:01.175874Z"
    }
   },
   "source": [
    "# Initialize EDA analyzer\n",
    "analyzer = ExploratoryDataAnalysis(settings)\n",
    "\n",
    "print(\"📊 Starting Exploratory Data Analysis...\")\n",
    "print(\"⏱️  This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Perform EDA\n",
    "eda_report = analyzer.perform_eda(\n",
    "    df=master_features,\n",
    "    target_col='churned',\n",
    "    generate_plots=True\n",
    ")\n",
    "\n",
    "print(\"\\n✅ EDA complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📊 DATA OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "overview = eda_report.get('data_overview', {})\n",
    "\n",
    "print(f\"\\nDataset Shape: {overview.get('shape', 'N/A')}\")\n",
    "print(f\"Memory Usage: {overview.get('memory_usage_mb', 0):.2f} MB\")\n",
    "print(f\"Numeric Features: {overview.get('numeric_features', 0)}\")\n",
    "print(f\"Categorical Features: {overview.get('categorical_features', 0)}\")\n",
    "print(f\"Duplicate Rows: {overview.get('duplicate_rows', 0):,}\")\n",
    "\n",
    "if overview.get('zero_variance_features'):\n",
    "    print(f\"\\n⚠️  Zero Variance Features: {len(overview['zero_variance_features'])}\")\n",
    "    \n",
    "if overview.get('high_cardinality_features'):\n",
    "    print(f\"⚠️  High Cardinality Features: {len(overview['high_cardinality_features'])}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Target Analysis (Churn)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n🎯 TARGET VARIABLE ANALYSIS (CHURN)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "target_analysis = eda_report.get('target_analysis', {})\n",
    "\n",
    "print(f\"\\nChurn Distribution:\")\n",
    "for label, count in target_analysis.get('distribution', {}).items():\n",
    "    pct = target_analysis.get('percentage', {}).get(label, 0)\n",
    "    print(f\"  • {label}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "if target_analysis.get('class_ratio'):\n",
    "    print(f\"\\nClass Imbalance Ratio: {target_analysis['class_ratio']:.2f}:1\")\n",
    "    \n",
    "if target_analysis.get('entropy'):\n",
    "    print(f\"Entropy: {target_analysis['entropy']:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Feature Correlations with Target"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:01.252064Z",
     "start_time": "2025-10-01T06:15:01.250732Z"
    }
   },
   "source": [
    "print(\"\\n📈 TOP FEATURES CORRELATED WITH CHURN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correlation_analysis = eda_report.get('correlation_analysis', {})\n",
    "target_correlations = correlation_analysis.get('target_correlations', {})\n",
    "\n",
    "if target_correlations:\n",
    "    # Show top 15 correlations\n",
    "    corr_df = pd.DataFrame(list(target_correlations.items())[:15], \n",
    "                          columns=['Feature', 'Correlation'])\n",
    "    corr_df['Abs_Correlation'] = corr_df['Correlation'].abs()\n",
    "    corr_df = corr_df.sort_values('Abs_Correlation', ascending=False)\n",
    "    \n",
    "    display(corr_df[['Feature', 'Correlation']].head(15))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_15 = corr_df.head(15)\n",
    "    colors = ['red' if x < 0 else 'green' for x in top_15['Correlation']]\n",
    "    plt.barh(range(len(top_15)), top_15['Correlation'].values, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(top_15)), top_15['Feature'].values)\n",
    "    plt.xlabel('Correlation with Churn')\n",
    "    plt.title('Top 15 Features Correlated with Churn')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No correlation data available\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:01.267206Z",
     "start_time": "2025-10-01T06:15:01.265107Z"
    }
   },
   "source": [
    "print(\"\\n👥 CHURN RATE BY CUSTOMER SEGMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "segmentation = eda_report.get('segmentation_analysis', {})\n",
    "\n",
    "# Customer Segment\n",
    "if 'by_customer_segment' in segmentation:\n",
    "    print(\"\\nBy Customer Segment:\")\n",
    "    segment_data = segmentation['by_customer_segment']\n",
    "    segment_df = pd.DataFrame(segment_data).T\n",
    "    if 'churned_mean' in segment_df.columns:\n",
    "        segment_df = segment_df.sort_values('churned_mean', ascending=False)\n",
    "        display(segment_df)\n",
    "\n",
    "# RFM Segment\n",
    "if 'by_rfm_segment' in segmentation:\n",
    "    print(\"\\nBy RFM Segment:\")\n",
    "    rfm_data = segmentation['by_rfm_segment']\n",
    "    rfm_df = pd.DataFrame(rfm_data).T\n",
    "    if 'churned_mean' in rfm_df.columns:\n",
    "        rfm_df = rfm_df.sort_values('churned_mean', ascending=False)\n",
    "        display(rfm_df.head(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📊 STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stat_tests = eda_report.get('statistical_tests', {})\n",
    "\n",
    "# T-tests for numeric features\n",
    "if 't_tests' in stat_tests:\n",
    "    print(\"\\nT-Tests (Top 10 Most Significant):\")\n",
    "    t_tests = stat_tests['t_tests']\n",
    "    if t_tests:\n",
    "        t_test_df = pd.DataFrame(t_tests)\n",
    "        t_test_df = t_test_df.sort_values('p_value')\n",
    "        display(t_test_df.head(10))\n",
    "\n",
    "# Chi-square tests for categorical features\n",
    "if 'chi_square_tests' in stat_tests:\n",
    "    print(\"\\nChi-Square Tests (Top 10 Most Significant):\")\n",
    "    chi_tests = stat_tests['chi_square_tests']\n",
    "    if chi_tests:\n",
    "        chi_df = pd.DataFrame(chi_tests)\n",
    "        chi_df = chi_df.sort_values('p_value')\n",
    "        display(chi_df.head(10))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 View Generated Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n📊 GENERATED VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if hasattr(analyzer, 'visualizations') and analyzer.visualizations:\n",
    "    print(f\"\\nTotal visualizations created: {len(analyzer.visualizations)}\")\n",
    "    print(\"\\nVisualization files:\")\n",
    "    for viz in analyzer.visualizations:\n",
    "        viz_path = Path(viz)\n",
    "        print(f\"  • {viz_path.name}\")\n",
    "        if viz_path.suffix == '.html':\n",
    "            print(f\"    → Open in browser: {viz_path}\")\n",
    "else:\n",
    "    print(\"No visualizations generated\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6️⃣ Model Preparation\n",
    "\n",
    "## Preparing Datasets for Phase 2\n",
    "\n",
    "Let's prepare our final datasets for machine learning modeling."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n🎯 PREPARING DATASETS FOR MODELING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Separate features and target\n",
    "target_col = 'churned'\n",
    "feature_cols = [col for col in master_features.columns \n",
    "                if col not in ['customer_id', target_col]]\n",
    "\n",
    "# Select only numeric features for initial modeling\n",
    "numeric_features = master_features[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "X = master_features[numeric_features]\n",
    "y = master_features[target_col]\n",
    "\n",
    "print(f\"\\nFeature Matrix (X):\")\n",
    "print(f\"  • Shape: {X.shape}\")\n",
    "print(f\"  • Features: {len(numeric_features)}\")\n",
    "print(f\"  • Memory: {X.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "print(f\"\\nTarget Vector (y):\")\n",
    "print(f\"  • Shape: {y.shape}\")\n",
    "print(f\"  • Churn rate: {y.mean()*100:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split into train/temp (70/30)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split temp into validation/test (15/15)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.5, \n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\n📊 DATASET SPLITS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "splits_info = [\n",
    "    ['Training Set', len(X_train), len(X_train)/len(X)*100, y_train.mean()*100],\n",
    "    ['Validation Set', len(X_val), len(X_val)/len(X)*100, y_val.mean()*100],\n",
    "    ['Test Set', len(X_test), len(X_test)/len(X)*100, y_test.mean()*100],\n",
    "    ['Total', len(X), 100.0, y.mean()*100]\n",
    "]\n",
    "\n",
    "splits_df = pd.DataFrame(splits_info, \n",
    "                        columns=['Dataset', 'Records', 'Percentage', 'Churn Rate (%)'])\n",
    "display(splits_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T06:15:01.303088Z",
     "start_time": "2025-10-01T06:15:01.300450Z"
    }
   },
   "source": [
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val),\n",
    "    columns=X_val.columns,\n",
    "    index=X_val.index\n",
    ")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Feature scaling complete!\")\n",
    "print(\"\\n📊 Scaled Feature Statistics:\")\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std:  {X_train_scaled.std().mean():.6f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Save Prepared Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save datasets to disk\n",
    "processed_dir = settings.paths.PROCESSED_DATA_DIR\n",
    "\n",
    "datasets = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_val': X_val_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "print(\"\\n💾 Saving datasets...\")\n",
    "for name, data in datasets.items():\n",
    "    filepath = processed_dir / f\"{name}.parquet\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        data.to_frame().to_parquet(filepath)\n",
    "    else:\n",
    "        data.to_parquet(filepath)\n",
    "    print(f\"  ✓ Saved {name} to {filepath.name}\")\n",
    "\n",
    "print(\"\\n✅ All datasets saved successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7️⃣ Complete Pipeline Execution\n",
    "\n",
    "## Running the Full Pipeline\n",
    "\n",
    "Now let's run the complete pipeline using the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = PipelineOrchestrator(settings)\n",
    "\n",
    "print(\"\\n🚀 RUNNING COMPLETE PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis will execute all stages:\")\n",
    "print(\"  1. Data Collection\")\n",
    "print(\"  2. Data Cleaning\")\n",
    "print(\"  3. Data Validation\")\n",
    "print(\"  4. Feature Engineering\")\n",
    "print(\"  5. Exploratory Analysis\")\n",
    "print(\"  6. Model Preparation\")\n",
    "print(\"\\n⏱️  Estimated time: 3-5 minutes\")\n",
    "print(\"\\nStarting pipeline...\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run the pipeline\n",
    "results = orchestrator.run_pipeline(\n",
    "    use_cache=True,\n",
    "    continue_on_error=False,\n",
    "    generate_plots=True,\n",
    "    deep_clean=True,\n",
    "    target_column='churned'\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Pipeline execution complete!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get pipeline summary\n",
    "summary = orchestrator.get_pipeline_summary()\n",
    "\n",
    "print(\"\\n📊 PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal Stages: {summary['total_stages']}\")\n",
    "print(f\"Successful: {summary['successful_stages']} ✅\")\n",
    "print(f\"Failed: {summary['failed_stages']} ❌\")\n",
    "print(f\"Total Time: {summary['total_execution_time']:.2f} seconds\")\n",
    "\n",
    "print(\"\\n📋 Stage Results:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stage_results = []\n",
    "for stage_name, stage_info in summary['stages'].items():\n",
    "    status = \"✅\" if stage_info['success'] else \"❌\"\n",
    "    stage_results.append([\n",
    "        status,\n",
    "        stage_name,\n",
    "        f\"{stage_info['execution_time']:.2f}s\",\n",
    "        stage_info['error'] if stage_info['error'] else \"-\"\n",
    "    ])\n",
    "\n",
    "results_df = pd.DataFrame(stage_results, \n",
    "                         columns=['Status', 'Stage', 'Time', 'Error'])\n",
    "display(results_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8️⃣ Final Summary and Next Steps\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "Let's review what we've achieved in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n🎉 PHASE 1 COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✅ Achievements:\")\n",
    "print(\"\\n1. Data Collection\")\n",
    "print(f\"   • Collected {len(raw_data)} datasets\")\n",
    "print(f\"   • Total records: {sum(len(df) for df in raw_data.values()):,}\")\n",
    "\n",
    "print(\"\\n2. Data Cleaning\")\n",
    "print(f\"   • Cleaned {len(cleaned_data)} datasets\")\n",
    "print(f\"   • Handled missing values, outliers, and duplicates\")\n",
    "\n",
    "print(\"\\n3. Feature Engineering\")\n",
    "print(f\"   • Created {len(master_features.columns)} features\")\n",
    "print(f\"   • From {len(master_features):,} customer records\")\n",
    "\n",
    "print(\"\\n4. Exploratory Analysis\")\n",
    "print(f\"   • Generated comprehensive EDA report\")\n",
    "print(f\"   • Created {len(analyzer.visualizations) if hasattr(analyzer, 'visualizations') else 0} visualizations\")\n",
    "\n",
    "print(\"\\n5. Model Preparation\")\n",
    "print(f\"   • Training set: {len(X_train):,} records\")\n",
    "print(f\"   • Validation set: {len(X_val):,} records\")\n",
    "print(f\"   • Test set: {len(X_test):,} records\")\n",
    "\n",
    "print(\"\\n\\n📁 Output Files:\")\n",
    "print(f\"   • Processed Data: {settings.paths.PROCESSED_DATA_DIR}\")\n",
    "print(f\"   • Reports: {settings.paths.REPORTS_DIR}\")\n",
    "print(f\"   • Visualizations: {settings.paths.FIGURES_DIR}\")\n",
    "\n",
    "print(\"\\n\\n🎯 Key Metrics:\")\n",
    "print(f\"   • Churn Rate: {y.mean()*100:.2f}%\")\n",
    "print(f\"   • Total Features: {len(numeric_features)}\")\n",
    "print(f\"   • Data Quality: ✅ Validated\")\n",
    "\n",
    "print(\"\\n\\n📈 Next Steps (Phase 2):\")\n",
    "print(\"   1. Model Selection\")\n",
    "print(\"      • Try XGBoost, Random Forest, Logistic Regression\")\n",
    "print(\"   2. Hyperparameter Tuning\")\n",
    "print(\"      • Optimize model parameters\")\n",
    "print(\"   3. Model Evaluation\")\n",
    "print(\"      • Target: 85%+ accuracy\")\n",
    "print(\"      • Focus on precision and recall\")\n",
    "print(\"   4. Model Deployment\")\n",
    "print(\"      • Create API endpoint\")\n",
    "print(\"      • Set up monitoring\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n💡 Tip: Review the interactive dashboard at:\")\n",
    "dashboard_path = settings.paths.FIGURES_DIR / 'interactive_dashboard.html'\n",
    "if dashboard_path.exists():\n",
    "    print(f\"   {dashboard_path}\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎓 Learning Summary\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "1. **Data Pipeline Architecture**: Modular, scalable design\n",
    "2. **Data Quality**: Importance of cleaning and validation\n",
    "3. **Feature Engineering**: Creating meaningful predictors\n",
    "4. **EDA**: Understanding data patterns and relationships\n",
    "5. **Model Preparation**: Train/val/test splits and scaling\n",
    "\n",
    "## Best Practices Demonstrated\n",
    "\n",
    "- ✅ Reproducible analysis with random seeds\n",
    "- ✅ Proper train/validation/test splits\n",
    "- ✅ Feature scaling before modeling\n",
    "- ✅ Comprehensive data validation\n",
    "- ✅ Documentation and reporting\n",
    "\n",
    "## Common Pitfalls Avoided\n",
    "\n",
    "- ❌ Data leakage between train/test\n",
    "- ❌ Ignoring class imbalance\n",
    "- ❌ Poor feature engineering\n",
    "- ❌ Inadequate data validation\n",
    "- ❌ Missing documentation\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've completed Phase 1 of the CRM Analytics Pipeline! 🎉**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
